{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from data_loader import Rescale\n",
    "from data_loader import RescaleT\n",
    "from data_loader import RandomCrop\n",
    "from data_loader import CenterCrop\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SalObjDataset\n",
    "\n",
    "from model import BASNet\n",
    "\n",
    "import pytorch_ssim\n",
    "import pytorch_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singhshivani158/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# ------- 1. define loss function --------\n",
    "\n",
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "ssim_loss = pytorch_ssim.SSIM(window_size=11,size_average=True)\n",
    "iou_loss = pytorch_iou.IOU(size_average=True)\n",
    "\n",
    "\n",
    "reduction='mean'\n",
    "\n",
    "def bce_ssim_loss(pred,target):\n",
    "\n",
    "\tbce_out = bce_loss(pred,target)\n",
    "\tssim_out = 1 - ssim_loss(pred,target)\n",
    "\tiou_out = iou_loss(pred,target)\n",
    "\n",
    "\tloss = bce_out + ssim_out + iou_out\n",
    "\n",
    "\treturn loss\n",
    "\n",
    "def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v):\n",
    "\n",
    "\tloss0 = bce_ssim_loss(d0,labels_v)\n",
    "\tloss1 = bce_ssim_loss(d1,labels_v)\n",
    "\tloss2 = bce_ssim_loss(d2,labels_v)\n",
    "\tloss3 = bce_ssim_loss(d3,labels_v)\n",
    "\tloss4 = bce_ssim_loss(d4,labels_v)\n",
    "\tloss5 = bce_ssim_loss(d5,labels_v)\n",
    "\tloss6 = bce_ssim_loss(d6,labels_v)\n",
    "\tloss7 = bce_ssim_loss(d7,labels_v)\n",
    "\t#ssim0 = 1 - ssim_loss(d0,labels_v)\n",
    "\n",
    "\t# iou0 = iou_loss(d0,labels_v)\n",
    "\t#loss = torch.pow(torch.mean(torch.abs(labels_v-d0)),2)*(5.0*loss0 + loss1 + loss2 + loss3 + loss4 + loss5) #+ 5.0*lossa\n",
    "\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7#+ 5.0*lossa\n",
    "\t#print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.item(),loss1.item(),loss2.item(),loss3.item(),loss4.item(),loss5.item(),loss6.item()))\t# print(\"BCE: l1:%3f, l2:%3f, l3:%3f, l4:%3f, l5:%3f, la:%3f, all:%3f\\n\"%(loss1.data[0],loss2.data[0],loss3.data[0],loss4.data[0],loss5.data[0],lossa.data[0],loss.data[0]))\n",
    "\n",
    "\treturn loss0, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 2. set the directory of training dataset --------\n",
    "\n",
    "data_dir = './train_data/'\n",
    "#tra_image_dir = 'DUTS/DUTS-TR/DUTS-TR/im_aug/'\n",
    "#tra_image_dir = 'DUTS/image/'\n",
    "tra_image_dir = 'NEW_DATA/image/'\n",
    "\n",
    "#tra_label_dir = 'DUTS/DUTS-TR/DUTS-TR/gt_aug/'\n",
    "#tra_label_dir = 'DUTS/mask/'\n",
    "tra_label_dir = 'NEW_DATA/mask/'\n",
    "\n",
    "image_ext = '.jpg'\n",
    "label_ext = '.png'\n",
    "\n",
    "model_dir = \"./saved_models/\"\n",
    "\n",
    "PATH = \"./saved_models/basnet.pth\"\n",
    "\n",
    "epoch_num = 200\n",
    "batch_size_train = 8 \n",
    "batch_size_val = 1\n",
    "train_num = 0\n",
    "val_num = 0\n",
    "\n",
    "tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basnet.pth  optimized_model.pth\n"
     ]
    }
   ],
   "source": [
    "! ls ./saved_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "train images:  3310\n",
      "train labels:  3310\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "\timg_name = img_path.split(\"/\")[-1]\n",
    "\n",
    "\taaa = img_name.split(\".\")\n",
    "\tbbb = aaa[0:-1]\n",
    "\timidx = bbb[0]\n",
    "\tfor i in range(1,len(bbb)):\n",
    "\t\timidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "\ttra_lbl_name_list.append(data_dir + tra_label_dir + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = round(len(tra_img_name_list) * 0.2)\n",
    "train_size = len(tra_img_name_list) - valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2648, 2648]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_size, train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "salobj_dataset = SalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(256),\n",
    "        RandomCrop(224),\n",
    "        ToTensorLab(flag=0)]))\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(salobj_dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(val_data, batch_size=batch_size_train, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 3. define model --------\n",
    "# define the net\n",
    "net = BASNet(3, 1)\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---define optimizer...\n",
      "---start training...\n"
     ]
    }
   ],
   "source": [
    "# ------- 4. define optimizer --------\n",
    "print(\"---define optimizer...\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# ------- 5. training process --------\n",
    "print(\"---start training...\")\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "\n",
    "\n",
    "ite_num_valid = 0\n",
    "running_loss_valid = 0.0\n",
    "\n",
    "running_tar_loss_valid = 0.0\n",
    "ite_num4val_valid = 0\n",
    "\n",
    "valid_loss_min = np.Inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singhshivani158/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/singhshivani158/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   1/200, batch:     8/ 2648, ite: 1] train loss: 8.998521, tar: 0.943130 ]\n",
      "[Valid loss: 1.222583, tar: 0.111891 ]\n",
      "*************\n",
      "Validation loss decreased (inf --> 0.111891).  Saving model ...\n",
      "##########\n",
      "[epoch:   1/200, batch:    16/ 2648, ite: 2] train loss: 7.917761, tar: 0.845558 ]\n",
      "[Valid loss: 2.381782, tar: 0.219096 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    24/ 2648, ite: 3] train loss: 11.055979, tar: 1.100870 ]\n",
      "[Valid loss: 3.459262, tar: 0.315325 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    32/ 2648, ite: 4] train loss: 10.583622, tar: 1.054717 ]\n",
      "[Valid loss: 4.449437, tar: 0.400819 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    40/ 2648, ite: 5] train loss: 9.086501, tar: 0.891874 ]\n",
      "[Valid loss: 5.355346, tar: 0.480542 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    48/ 2648, ite: 6] train loss: 8.429366, tar: 0.815002 ]\n",
      "[Valid loss: 6.215232, tar: 0.556442 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    56/ 2648, ite: 7] train loss: 8.349116, tar: 0.793718 ]\n",
      "[Valid loss: 7.069030, tar: 0.637118 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    64/ 2648, ite: 8] train loss: 7.658973, tar: 0.729226 ]\n",
      "[Valid loss: 7.917581, tar: 0.720444 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    72/ 2648, ite: 9] train loss: 7.611280, tar: 0.742274 ]\n",
      "[Valid loss: 8.770049, tar: 0.804916 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    80/ 2648, ite: 10] train loss: 7.292981, tar: 0.724935 ]\n",
      "[Valid loss: 9.595333, tar: 0.885979 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    88/ 2648, ite: 11] train loss: 6.919305, tar: 0.691181 ]\n",
      "[Valid loss: 10.412768, tar: 0.964089 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:    96/ 2648, ite: 12] train loss: 6.658838, tar: 0.661818 ]\n",
      "[Valid loss: 11.240598, tar: 1.041174 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   104/ 2648, ite: 13] train loss: 6.651904, tar: 0.665706 ]\n",
      "[Valid loss: 12.062522, tar: 1.116917 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   112/ 2648, ite: 14] train loss: 6.624907, tar: 0.659255 ]\n",
      "[Valid loss: 12.877780, tar: 1.190744 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   120/ 2648, ite: 15] train loss: 6.539982, tar: 0.647211 ]\n",
      "[Valid loss: 13.669349, tar: 1.261514 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   128/ 2648, ite: 16] train loss: 6.242796, tar: 0.614860 ]\n",
      "[Valid loss: 14.463843, tar: 1.333041 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   136/ 2648, ite: 17] train loss: 6.145944, tar: 0.604427 ]\n",
      "[Valid loss: 15.264982, tar: 1.405782 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   144/ 2648, ite: 18] train loss: 6.108842, tar: 0.601717 ]\n",
      "[Valid loss: 16.093772, tar: 1.480590 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   152/ 2648, ite: 19] train loss: 6.106386, tar: 0.602773 ]\n",
      "[Valid loss: 16.925177, tar: 1.556558 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   160/ 2648, ite: 20] train loss: 5.963609, tar: 0.586595 ]\n",
      "[Valid loss: 17.745199, tar: 1.630174 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   168/ 2648, ite: 21] train loss: 6.271597, tar: 0.611380 ]\n",
      "[Valid loss: 18.557777, tar: 1.704249 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   176/ 2648, ite: 22] train loss: 6.292671, tar: 0.613733 ]\n",
      "[Valid loss: 19.362263, tar: 1.777398 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   184/ 2648, ite: 23] train loss: 6.187710, tar: 0.601699 ]\n",
      "[Valid loss: 20.156359, tar: 1.849871 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   192/ 2648, ite: 24] train loss: 6.172388, tar: 0.599980 ]\n",
      "[Valid loss: 20.953364, tar: 1.923656 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   200/ 2648, ite: 25] train loss: 6.204131, tar: 0.599980 ]\n",
      "[Valid loss: 21.720425, tar: 1.995011 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   208/ 2648, ite: 26] train loss: 6.165149, tar: 0.596412 ]\n",
      "[Valid loss: 22.479748, tar: 2.066966 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   216/ 2648, ite: 27] train loss: 6.122652, tar: 0.591571 ]\n",
      "[Valid loss: 23.224059, tar: 2.138334 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   224/ 2648, ite: 28] train loss: 6.289928, tar: 0.609941 ]\n",
      "[Valid loss: 23.992168, tar: 2.213719 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   232/ 2648, ite: 29] train loss: 6.251390, tar: 0.607594 ]\n",
      "[Valid loss: 24.775887, tar: 2.292112 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   240/ 2648, ite: 30] train loss: 6.179286, tar: 0.601172 ]\n",
      "[Valid loss: 25.547833, tar: 2.370873 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   248/ 2648, ite: 31] train loss: 6.122049, tar: 0.598799 ]\n",
      "[Valid loss: 26.327753, tar: 2.450080 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   256/ 2648, ite: 32] train loss: 6.042358, tar: 0.590715 ]\n",
      "[Valid loss: 27.104463, tar: 2.529149 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   264/ 2648, ite: 33] train loss: 5.974812, tar: 0.584680 ]\n",
      "[Valid loss: 27.874704, tar: 2.606490 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   272/ 2648, ite: 34] train loss: 5.918614, tar: 0.580007 ]\n",
      "[Valid loss: 28.636609, tar: 2.681933 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   280/ 2648, ite: 35] train loss: 5.947910, tar: 0.583803 ]\n",
      "[Valid loss: 29.371385, tar: 2.754067 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   288/ 2648, ite: 36] train loss: 5.898202, tar: 0.578021 ]\n",
      "[Valid loss: 30.114060, tar: 2.827512 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   296/ 2648, ite: 37] train loss: 5.829103, tar: 0.572474 ]\n",
      "[Valid loss: 30.848166, tar: 2.900171 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   304/ 2648, ite: 38] train loss: 5.787621, tar: 0.567999 ]\n",
      "[Valid loss: 31.578353, tar: 2.972183 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   312/ 2648, ite: 39] train loss: 5.740655, tar: 0.562733 ]\n",
      "[Valid loss: 32.314753, tar: 3.044320 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   320/ 2648, ite: 40] train loss: 5.748485, tar: 0.560923 ]\n",
      "[Valid loss: 33.046391, tar: 3.116966 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   328/ 2648, ite: 41] train loss: 5.733903, tar: 0.560046 ]\n",
      "[Valid loss: 33.769295, tar: 3.187886 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   336/ 2648, ite: 42] train loss: 5.681232, tar: 0.554537 ]\n",
      "[Valid loss: 34.490672, tar: 3.259597 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   344/ 2648, ite: 43] train loss: 5.660409, tar: 0.552296 ]\n",
      "[Valid loss: 35.232273, tar: 3.333523 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   352/ 2648, ite: 44] train loss: 5.682941, tar: 0.554868 ]\n",
      "[Valid loss: 35.967363, tar: 3.407539 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   360/ 2648, ite: 45] train loss: 5.749953, tar: 0.567182 ]\n",
      "[Valid loss: 36.680292, tar: 3.477883 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   368/ 2648, ite: 46] train loss: 5.710575, tar: 0.563381 ]\n",
      "[Valid loss: 37.394437, tar: 3.547586 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   376/ 2648, ite: 47] train loss: 5.861583, tar: 0.576290 ]\n",
      "[Valid loss: 38.109230, tar: 3.617198 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   384/ 2648, ite: 48] train loss: 5.843157, tar: 0.574973 ]\n",
      "[Valid loss: 38.815002, tar: 3.685235 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   392/ 2648, ite: 49] train loss: 5.878308, tar: 0.580310 ]\n",
      "[Valid loss: 39.526021, tar: 3.754300 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   400/ 2648, ite: 50] train loss: 5.817952, tar: 0.572977 ]\n",
      "[Valid loss: 40.218758, tar: 3.821524 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   408/ 2648, ite: 51] train loss: 5.767821, tar: 0.568110 ]\n",
      "[Valid loss: 40.902319, tar: 3.888239 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   416/ 2648, ite: 52] train loss: 5.724137, tar: 0.563590 ]\n",
      "[Valid loss: 41.587226, tar: 3.956139 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   424/ 2648, ite: 53] train loss: 5.753058, tar: 0.567930 ]\n",
      "[Valid loss: 42.292843, tar: 4.026153 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   432/ 2648, ite: 54] train loss: 5.772913, tar: 0.568878 ]\n",
      "[Valid loss: 42.977394, tar: 4.094054 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   440/ 2648, ite: 55] train loss: 5.727166, tar: 0.564370 ]\n",
      "[Valid loss: 43.661777, tar: 4.161671 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   448/ 2648, ite: 56] train loss: 5.742891, tar: 0.565527 ]\n",
      "[Valid loss: 44.359703, tar: 4.230937 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   456/ 2648, ite: 57] train loss: 5.777769, tar: 0.571225 ]\n",
      "[Valid loss: 45.030180, tar: 4.296842 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   464/ 2648, ite: 58] train loss: 5.730970, tar: 0.565604 ]\n",
      "[Valid loss: 45.712838, tar: 4.363213 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   472/ 2648, ite: 59] train loss: 5.691277, tar: 0.562499 ]\n",
      "[Valid loss: 46.398970, tar: 4.430372 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   480/ 2648, ite: 60] train loss: 5.639459, tar: 0.557138 ]\n",
      "[Valid loss: 47.092569, tar: 4.497398 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   488/ 2648, ite: 61] train loss: 5.589184, tar: 0.551588 ]\n",
      "[Valid loss: 47.773624, tar: 4.563101 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   496/ 2648, ite: 62] train loss: 5.588394, tar: 0.550654 ]\n",
      "[Valid loss: 48.451784, tar: 4.627972 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   504/ 2648, ite: 63] train loss: 5.551848, tar: 0.546802 ]\n",
      "[Valid loss: 49.124612, tar: 4.692910 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   512/ 2648, ite: 64] train loss: 5.585814, tar: 0.549003 ]\n",
      "[Valid loss: 49.793784, tar: 4.758046 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   520/ 2648, ite: 65] train loss: 5.625243, tar: 0.553499 ]\n",
      "[Valid loss: 50.469738, tar: 4.825062 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   528/ 2648, ite: 66] train loss: 5.581854, tar: 0.548967 ]\n",
      "[Valid loss: 51.130483, tar: 4.890540 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   536/ 2648, ite: 67] train loss: 5.586454, tar: 0.549146 ]\n",
      "[Valid loss: 51.793916, tar: 4.957226 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   544/ 2648, ite: 68] train loss: 5.588038, tar: 0.548583 ]\n",
      "[Valid loss: 52.460877, tar: 5.024810 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   552/ 2648, ite: 69] train loss: 5.600213, tar: 0.550059 ]\n",
      "[Valid loss: 53.134726, tar: 5.093023 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   560/ 2648, ite: 70] train loss: 5.589758, tar: 0.550323 ]\n",
      "[Valid loss: 53.816857, tar: 5.162089 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   568/ 2648, ite: 71] train loss: 5.645835, tar: 0.556378 ]\n",
      "[Valid loss: 54.475218, tar: 5.227368 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   576/ 2648, ite: 72] train loss: 5.629988, tar: 0.554920 ]\n",
      "[Valid loss: 55.155247, tar: 5.295267 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   584/ 2648, ite: 73] train loss: 5.647747, tar: 0.555696 ]\n",
      "[Valid loss: 55.816283, tar: 5.361383 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   592/ 2648, ite: 74] train loss: 5.632729, tar: 0.554276 ]\n",
      "[Valid loss: 56.489434, tar: 5.431100 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   600/ 2648, ite: 75] train loss: 5.622350, tar: 0.554038 ]\n",
      "[Valid loss: 57.176546, tar: 5.503404 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   608/ 2648, ite: 76] train loss: 5.595653, tar: 0.550984 ]\n",
      "[Valid loss: 57.884405, tar: 5.580489 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   616/ 2648, ite: 77] train loss: 5.560938, tar: 0.547935 ]\n",
      "[Valid loss: 58.590759, tar: 5.658373 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   624/ 2648, ite: 78] train loss: 5.528849, tar: 0.544887 ]\n",
      "[Valid loss: 59.307650, tar: 5.737846 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   632/ 2648, ite: 79] train loss: 5.556041, tar: 0.550204 ]\n",
      "[Valid loss: 60.023605, tar: 5.815087 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   640/ 2648, ite: 80] train loss: 5.539573, tar: 0.548169 ]\n",
      "[Valid loss: 60.739242, tar: 5.890958 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   648/ 2648, ite: 81] train loss: 5.550768, tar: 0.550008 ]\n",
      "[Valid loss: 61.426239, tar: 5.961162 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   656/ 2648, ite: 82] train loss: 5.523697, tar: 0.546904 ]\n",
      "[Valid loss: 62.116001, tar: 6.030594 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   664/ 2648, ite: 83] train loss: 5.524618, tar: 0.547597 ]\n",
      "[Valid loss: 62.789080, tar: 6.097567 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   672/ 2648, ite: 84] train loss: 5.522826, tar: 0.547445 ]\n",
      "[Valid loss: 63.451653, tar: 6.165050 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   680/ 2648, ite: 85] train loss: 5.521018, tar: 0.547084 ]\n",
      "[Valid loss: 64.144245, tar: 6.237893 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   688/ 2648, ite: 86] train loss: 5.512349, tar: 0.546276 ]\n",
      "[Valid loss: 64.857136, tar: 6.315886 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   696/ 2648, ite: 87] train loss: 5.485520, tar: 0.543839 ]\n",
      "[Valid loss: 65.578513, tar: 6.397548 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   704/ 2648, ite: 88] train loss: 5.515058, tar: 0.549131 ]\n",
      "[Valid loss: 66.331176, tar: 6.482800 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   712/ 2648, ite: 89] train loss: 5.517141, tar: 0.550277 ]\n",
      "[Valid loss: 67.064124, tar: 6.563957 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   720/ 2648, ite: 90] train loss: 5.512256, tar: 0.551224 ]\n",
      "[Valid loss: 67.773195, tar: 6.639099 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   728/ 2648, ite: 91] train loss: 5.490465, tar: 0.549198 ]\n",
      "[Valid loss: 68.461889, tar: 6.710009 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   736/ 2648, ite: 92] train loss: 5.480633, tar: 0.548104 ]\n",
      "[Valid loss: 69.134313, tar: 6.776245 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   744/ 2648, ite: 93] train loss: 5.483100, tar: 0.548696 ]\n",
      "[Valid loss: 69.802249, tar: 6.840868 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   752/ 2648, ite: 94] train loss: 5.493243, tar: 0.550172 ]\n",
      "[Valid loss: 70.471704, tar: 6.906322 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   760/ 2648, ite: 95] train loss: 5.466539, tar: 0.546636 ]\n",
      "[Valid loss: 71.134939, tar: 6.970336 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   768/ 2648, ite: 96] train loss: 5.499611, tar: 0.549318 ]\n",
      "[Valid loss: 71.798479, tar: 7.033931 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   776/ 2648, ite: 97] train loss: 5.537537, tar: 0.554022 ]\n",
      "[Valid loss: 72.462074, tar: 7.098168 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   784/ 2648, ite: 98] train loss: 5.518453, tar: 0.552194 ]\n",
      "[Valid loss: 73.134333, tar: 7.163993 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   792/ 2648, ite: 99] train loss: 5.498662, tar: 0.549748 ]\n",
      "[Valid loss: 73.810593, tar: 7.230027 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   800/ 2648, ite: 100] train loss: 5.477850, tar: 0.547474 ]\n",
      "[Valid loss: 74.479774, tar: 7.295200 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   808/ 2648, ite: 101] train loss: 5.501052, tar: 0.550401 ]\n",
      "[Valid loss: 75.138622, tar: 7.359512 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   816/ 2648, ite: 102] train loss: 5.517384, tar: 0.552419 ]\n",
      "[Valid loss: 75.799452, tar: 7.423629 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   824/ 2648, ite: 103] train loss: 5.517898, tar: 0.552881 ]\n",
      "[Valid loss: 76.455588, tar: 7.487729 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   832/ 2648, ite: 104] train loss: 5.536438, tar: 0.555379 ]\n",
      "[Valid loss: 77.117284, tar: 7.552498 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   840/ 2648, ite: 105] train loss: 5.528565, tar: 0.554727 ]\n",
      "[Valid loss: 77.764759, tar: 7.615617 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   848/ 2648, ite: 106] train loss: 5.538685, tar: 0.556123 ]\n",
      "[Valid loss: 78.410577, tar: 7.678718 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   856/ 2648, ite: 107] train loss: 5.554188, tar: 0.558224 ]\n",
      "[Valid loss: 79.060009, tar: 7.742814 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   864/ 2648, ite: 108] train loss: 5.539632, tar: 0.556869 ]\n",
      "[Valid loss: 79.711388, tar: 7.807564 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   872/ 2648, ite: 109] train loss: 5.517321, tar: 0.554530 ]\n",
      "[Valid loss: 80.361610, tar: 7.871940 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   880/ 2648, ite: 110] train loss: 5.516794, tar: 0.554398 ]\n",
      "[Valid loss: 81.018438, tar: 7.936848 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   888/ 2648, ite: 111] train loss: 5.496951, tar: 0.551843 ]\n",
      "[Valid loss: 81.671662, tar: 8.001866 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   896/ 2648, ite: 112] train loss: 5.485837, tar: 0.550443 ]\n",
      "[Valid loss: 82.321957, tar: 8.066025 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   904/ 2648, ite: 113] train loss: 5.509252, tar: 0.552749 ]\n",
      "[Valid loss: 82.977306, tar: 8.131940 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   912/ 2648, ite: 114] train loss: 5.492988, tar: 0.551008 ]\n",
      "[Valid loss: 83.623358, tar: 8.196498 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   920/ 2648, ite: 115] train loss: 5.519591, tar: 0.553761 ]\n",
      "[Valid loss: 84.281588, tar: 8.262751 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   928/ 2648, ite: 116] train loss: 5.543415, tar: 0.556476 ]\n",
      "[Valid loss: 84.930462, tar: 8.328584 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   936/ 2648, ite: 117] train loss: 5.529854, tar: 0.554959 ]\n",
      "[Valid loss: 85.580266, tar: 8.393896 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   944/ 2648, ite: 118] train loss: 5.534527, tar: 0.555653 ]\n",
      "[Valid loss: 86.223688, tar: 8.458266 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   952/ 2648, ite: 119] train loss: 5.527829, tar: 0.554499 ]\n",
      "[Valid loss: 86.865916, tar: 8.522249 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   960/ 2648, ite: 120] train loss: 5.508824, tar: 0.552481 ]\n",
      "[Valid loss: 87.507876, tar: 8.586550 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   968/ 2648, ite: 121] train loss: 5.503521, tar: 0.551780 ]\n",
      "[Valid loss: 88.132572, tar: 8.648855 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   976/ 2648, ite: 122] train loss: 5.485550, tar: 0.549781 ]\n",
      "[Valid loss: 88.767252, tar: 8.712101 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   984/ 2648, ite: 123] train loss: 5.517009, tar: 0.552702 ]\n",
      "[Valid loss: 89.407950, tar: 8.775840 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:   992/ 2648, ite: 124] train loss: 5.518275, tar: 0.553327 ]\n",
      "[Valid loss: 90.045513, tar: 8.839876 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1000/ 2648, ite: 125] train loss: 5.507859, tar: 0.551882 ]\n",
      "[Valid loss: 90.688033, tar: 8.904338 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1008/ 2648, ite: 126] train loss: 5.492121, tar: 0.549968 ]\n",
      "[Valid loss: 91.315744, tar: 8.966609 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1016/ 2648, ite: 127] train loss: 5.475549, tar: 0.548151 ]\n",
      "[Valid loss: 91.952570, tar: 9.030168 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1024/ 2648, ite: 128] train loss: 5.514772, tar: 0.552588 ]\n",
      "[Valid loss: 92.570929, tar: 9.091350 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1032/ 2648, ite: 129] train loss: 5.496736, tar: 0.550768 ]\n",
      "[Valid loss: 93.201614, tar: 9.154378 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1040/ 2648, ite: 130] train loss: 5.513784, tar: 0.552357 ]\n",
      "[Valid loss: 93.832368, tar: 9.217315 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1048/ 2648, ite: 131] train loss: 5.492936, tar: 0.549881 ]\n",
      "[Valid loss: 94.453328, tar: 9.279552 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1056/ 2648, ite: 132] train loss: 5.505198, tar: 0.551291 ]\n",
      "[Valid loss: 95.089123, tar: 9.344038 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1064/ 2648, ite: 133] train loss: 5.522007, tar: 0.553976 ]\n",
      "[Valid loss: 95.725100, tar: 9.409261 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1072/ 2648, ite: 134] train loss: 5.501106, tar: 0.551803 ]\n",
      "[Valid loss: 96.366557, tar: 9.474970 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1080/ 2648, ite: 135] train loss: 5.515237, tar: 0.553138 ]\n",
      "[Valid loss: 97.024565, tar: 9.542611 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1088/ 2648, ite: 136] train loss: 5.509497, tar: 0.552664 ]\n",
      "[Valid loss: 97.699213, tar: 9.611638 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1096/ 2648, ite: 137] train loss: 5.492217, tar: 0.550706 ]\n",
      "[Valid loss: 98.376214, tar: 9.680807 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1104/ 2648, ite: 138] train loss: 5.486951, tar: 0.550381 ]\n",
      "[Valid loss: 99.078366, tar: 9.752942 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1112/ 2648, ite: 139] train loss: 5.504618, tar: 0.552271 ]\n",
      "[Valid loss: 99.757302, tar: 9.822108 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1120/ 2648, ite: 140] train loss: 5.486109, tar: 0.550111 ]\n",
      "[Valid loss: 100.416983, tar: 9.888645 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1128/ 2648, ite: 141] train loss: 5.495930, tar: 0.551921 ]\n",
      "[Valid loss: 101.063556, tar: 9.953517 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1136/ 2648, ite: 142] train loss: 5.498007, tar: 0.552149 ]\n",
      "[Valid loss: 101.702324, tar: 10.017322 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1144/ 2648, ite: 143] train loss: 5.486402, tar: 0.550910 ]\n",
      "[Valid loss: 102.330342, tar: 10.079834 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1152/ 2648, ite: 144] train loss: 5.507112, tar: 0.553538 ]\n",
      "[Valid loss: 102.947662, tar: 10.141378 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1160/ 2648, ite: 145] train loss: 5.509677, tar: 0.553952 ]\n",
      "[Valid loss: 103.558112, tar: 10.202686 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1168/ 2648, ite: 146] train loss: 5.508421, tar: 0.553960 ]\n",
      "[Valid loss: 104.186355, tar: 10.266592 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1176/ 2648, ite: 147] train loss: 5.504651, tar: 0.553732 ]\n",
      "[Valid loss: 104.815047, tar: 10.330286 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1184/ 2648, ite: 148] train loss: 5.496366, tar: 0.552651 ]\n",
      "[Valid loss: 105.451958, tar: 10.394808 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1192/ 2648, ite: 149] train loss: 5.487889, tar: 0.552106 ]\n",
      "[Valid loss: 106.072195, tar: 10.457698 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1200/ 2648, ite: 150] train loss: 5.480296, tar: 0.551308 ]\n",
      "[Valid loss: 106.697367, tar: 10.520614 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1208/ 2648, ite: 151] train loss: 5.476153, tar: 0.550772 ]\n",
      "[Valid loss: 107.314615, tar: 10.582638 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1216/ 2648, ite: 152] train loss: 5.476459, tar: 0.550784 ]\n",
      "[Valid loss: 107.933501, tar: 10.644590 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1224/ 2648, ite: 153] train loss: 5.489545, tar: 0.552707 ]\n",
      "[Valid loss: 108.558197, tar: 10.706961 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1232/ 2648, ite: 154] train loss: 5.487084, tar: 0.552273 ]\n",
      "[Valid loss: 109.176253, tar: 10.769031 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1240/ 2648, ite: 155] train loss: 5.513773, tar: 0.555592 ]\n",
      "[Valid loss: 109.797927, tar: 10.831672 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1248/ 2648, ite: 156] train loss: 5.514785, tar: 0.555948 ]\n",
      "[Valid loss: 110.416865, tar: 10.893805 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1256/ 2648, ite: 157] train loss: 5.515522, tar: 0.556302 ]\n",
      "[Valid loss: 111.033825, tar: 10.956088 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1264/ 2648, ite: 158] train loss: 5.498246, tar: 0.554304 ]\n",
      "[Valid loss: 111.659932, tar: 11.019329 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1272/ 2648, ite: 159] train loss: 5.494785, tar: 0.553710 ]\n",
      "[Valid loss: 112.288882, tar: 11.083246 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1280/ 2648, ite: 160] train loss: 5.493272, tar: 0.553496 ]\n",
      "[Valid loss: 112.927830, tar: 11.148828 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1288/ 2648, ite: 161] train loss: 5.492799, tar: 0.553484 ]\n",
      "[Valid loss: 113.555119, tar: 11.212638 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1296/ 2648, ite: 162] train loss: 5.481279, tar: 0.552329 ]\n",
      "[Valid loss: 114.186174, tar: 11.276666 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1304/ 2648, ite: 163] train loss: 5.468138, tar: 0.550899 ]\n",
      "[Valid loss: 114.805419, tar: 11.339350 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1312/ 2648, ite: 164] train loss: 5.457550, tar: 0.549734 ]\n",
      "[Valid loss: 115.434705, tar: 11.402654 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1320/ 2648, ite: 165] train loss: 5.452147, tar: 0.549516 ]\n",
      "[Valid loss: 116.056724, tar: 11.464964 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1328/ 2648, ite: 166] train loss: 5.445268, tar: 0.548752 ]\n",
      "[Valid loss: 116.675404, tar: 11.526401 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1336/ 2648, ite: 167] train loss: 5.443064, tar: 0.548661 ]\n",
      "[Valid loss: 117.293955, tar: 11.587423 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1344/ 2648, ite: 168] train loss: 5.442368, tar: 0.548578 ]\n",
      "[Valid loss: 117.896992, tar: 11.646615 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1352/ 2648, ite: 169] train loss: 5.464006, tar: 0.550557 ]\n",
      "[Valid loss: 118.522059, tar: 11.708561 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1360/ 2648, ite: 170] train loss: 5.448650, tar: 0.549136 ]\n",
      "[Valid loss: 119.139476, tar: 11.769382 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1368/ 2648, ite: 171] train loss: 5.434602, tar: 0.547200 ]\n",
      "[Valid loss: 119.761266, tar: 11.830704 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1376/ 2648, ite: 172] train loss: 5.435827, tar: 0.547370 ]\n",
      "[Valid loss: 120.388007, tar: 11.892912 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1384/ 2648, ite: 173] train loss: 5.424105, tar: 0.546132 ]\n",
      "[Valid loss: 121.034985, tar: 11.957123 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1392/ 2648, ite: 174] train loss: 5.414312, tar: 0.545039 ]\n",
      "[Valid loss: 121.670978, tar: 12.019826 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1400/ 2648, ite: 175] train loss: 5.411628, tar: 0.544892 ]\n",
      "[Valid loss: 122.319778, tar: 12.083518 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1408/ 2648, ite: 176] train loss: 5.403359, tar: 0.544193 ]\n",
      "[Valid loss: 122.961024, tar: 12.146159 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1416/ 2648, ite: 177] train loss: 5.395716, tar: 0.543351 ]\n",
      "[Valid loss: 123.614095, tar: 12.209706 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1424/ 2648, ite: 178] train loss: 5.411360, tar: 0.544379 ]\n",
      "[Valid loss: 124.264471, tar: 12.272878 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1432/ 2648, ite: 179] train loss: 5.405779, tar: 0.543799 ]\n",
      "[Valid loss: 124.908846, tar: 12.335625 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1440/ 2648, ite: 180] train loss: 5.390404, tar: 0.541892 ]\n",
      "[Valid loss: 125.551398, tar: 12.397951 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1448/ 2648, ite: 181] train loss: 5.389286, tar: 0.541825 ]\n",
      "[Valid loss: 126.199962, tar: 12.461304 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1456/ 2648, ite: 182] train loss: 5.382728, tar: 0.541215 ]\n",
      "[Valid loss: 126.845764, tar: 12.524011 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1464/ 2648, ite: 183] train loss: 5.368131, tar: 0.539614 ]\n",
      "[Valid loss: 127.500851, tar: 12.587932 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1472/ 2648, ite: 184] train loss: 5.358821, tar: 0.538838 ]\n",
      "[Valid loss: 128.144970, tar: 12.650907 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1480/ 2648, ite: 185] train loss: 5.342625, tar: 0.537100 ]\n",
      "[Valid loss: 128.779926, tar: 12.712786 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1488/ 2648, ite: 186] train loss: 5.327646, tar: 0.535425 ]\n",
      "[Valid loss: 129.425768, tar: 12.775950 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1496/ 2648, ite: 187] train loss: 5.320183, tar: 0.534441 ]\n",
      "[Valid loss: 130.076085, tar: 12.839805 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1504/ 2648, ite: 188] train loss: 5.306302, tar: 0.532821 ]\n",
      "[Valid loss: 130.717570, tar: 12.902442 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1512/ 2648, ite: 189] train loss: 5.303545, tar: 0.532693 ]\n",
      "[Valid loss: 131.375688, tar: 12.966557 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1520/ 2648, ite: 190] train loss: 5.300585, tar: 0.532283 ]\n",
      "[Valid loss: 132.025619, tar: 13.030307 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1528/ 2648, ite: 191] train loss: 5.287451, tar: 0.530720 ]\n",
      "[Valid loss: 132.681507, tar: 13.094557 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1536/ 2648, ite: 192] train loss: 5.278746, tar: 0.529936 ]\n",
      "[Valid loss: 133.346006, tar: 13.159476 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1544/ 2648, ite: 193] train loss: 5.264466, tar: 0.528197 ]\n",
      "[Valid loss: 134.003511, tar: 13.224001 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1552/ 2648, ite: 194] train loss: 5.261690, tar: 0.527891 ]\n",
      "[Valid loss: 134.671913, tar: 13.289835 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1560/ 2648, ite: 195] train loss: 5.245421, tar: 0.526212 ]\n",
      "[Valid loss: 135.329434, tar: 13.354070 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1568/ 2648, ite: 196] train loss: 5.252430, tar: 0.527269 ]\n",
      "[Valid loss: 135.982811, tar: 13.418150 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1576/ 2648, ite: 197] train loss: 5.256014, tar: 0.527885 ]\n",
      "[Valid loss: 136.637940, tar: 13.482340 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1584/ 2648, ite: 198] train loss: 5.264000, tar: 0.528591 ]\n",
      "[Valid loss: 137.290162, tar: 13.545906 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1592/ 2648, ite: 199] train loss: 5.250277, tar: 0.527103 ]\n",
      "[Valid loss: 137.949835, tar: 13.610341 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1600/ 2648, ite: 200] train loss: 5.236924, tar: 0.525676 ]\n",
      "[Valid loss: 138.607091, tar: 13.674503 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1608/ 2648, ite: 201] train loss: 5.225860, tar: 0.524226 ]\n",
      "[Valid loss: 139.236199, tar: 13.735040 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1616/ 2648, ite: 202] train loss: 5.231262, tar: 0.524591 ]\n",
      "[Valid loss: 139.881970, tar: 13.797640 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1624/ 2648, ite: 203] train loss: 5.225110, tar: 0.523789 ]\n",
      "[Valid loss: 140.529394, tar: 13.860436 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1632/ 2648, ite: 204] train loss: 5.225859, tar: 0.524068 ]\n",
      "[Valid loss: 141.151967, tar: 13.920825 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1640/ 2648, ite: 205] train loss: 5.214736, tar: 0.522854 ]\n",
      "[Valid loss: 141.784299, tar: 13.982118 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1648/ 2648, ite: 206] train loss: 5.209327, tar: 0.522187 ]\n",
      "[Valid loss: 142.405707, tar: 14.042790 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1656/ 2648, ite: 207] train loss: 5.225544, tar: 0.524259 ]\n",
      "[Valid loss: 143.027911, tar: 14.103882 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1664/ 2648, ite: 208] train loss: 5.212671, tar: 0.522749 ]\n",
      "[Valid loss: 143.651357, tar: 14.165649 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1672/ 2648, ite: 209] train loss: 5.210460, tar: 0.522479 ]\n",
      "[Valid loss: 144.270170, tar: 14.226780 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1680/ 2648, ite: 210] train loss: 5.205326, tar: 0.522082 ]\n",
      "[Valid loss: 144.901931, tar: 14.290188 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1688/ 2648, ite: 211] train loss: 5.211161, tar: 0.523019 ]\n",
      "[Valid loss: 145.530294, tar: 14.353200 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1696/ 2648, ite: 212] train loss: 5.209805, tar: 0.522705 ]\n",
      "[Valid loss: 146.163384, tar: 14.417033 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1704/ 2648, ite: 213] train loss: 5.209267, tar: 0.523067 ]\n",
      "[Valid loss: 146.793164, tar: 14.480274 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1712/ 2648, ite: 214] train loss: 5.202563, tar: 0.522218 ]\n",
      "[Valid loss: 147.435287, tar: 14.545094 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1720/ 2648, ite: 215] train loss: 5.191512, tar: 0.520771 ]\n",
      "[Valid loss: 148.080352, tar: 14.610278 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1728/ 2648, ite: 216] train loss: 5.192687, tar: 0.520799 ]\n",
      "[Valid loss: 148.718213, tar: 14.674140 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1736/ 2648, ite: 217] train loss: 5.185758, tar: 0.519943 ]\n",
      "[Valid loss: 149.353878, tar: 14.737108 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1744/ 2648, ite: 218] train loss: 5.175471, tar: 0.518710 ]\n",
      "[Valid loss: 149.987346, tar: 14.800341 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1752/ 2648, ite: 219] train loss: 5.172853, tar: 0.518504 ]\n",
      "[Valid loss: 150.613211, tar: 14.862295 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1760/ 2648, ite: 220] train loss: 5.182723, tar: 0.519782 ]\n",
      "[Valid loss: 151.239884, tar: 14.924641 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1768/ 2648, ite: 221] train loss: 5.178420, tar: 0.519392 ]\n",
      "[Valid loss: 151.864064, tar: 14.986306 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1776/ 2648, ite: 222] train loss: 5.166950, tar: 0.517990 ]\n",
      "[Valid loss: 152.493218, tar: 15.048899 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1784/ 2648, ite: 223] train loss: 5.179996, tar: 0.519332 ]\n",
      "[Valid loss: 153.114719, tar: 15.110192 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1792/ 2648, ite: 224] train loss: 5.204023, tar: 0.522234 ]\n",
      "[Valid loss: 153.730767, tar: 15.170834 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1800/ 2648, ite: 225] train loss: 5.212177, tar: 0.523365 ]\n",
      "[Valid loss: 154.361548, tar: 15.233099 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1808/ 2648, ite: 226] train loss: 5.206577, tar: 0.522664 ]\n",
      "[Valid loss: 154.981151, tar: 15.293985 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1816/ 2648, ite: 227] train loss: 5.201283, tar: 0.521880 ]\n",
      "[Valid loss: 155.607318, tar: 15.355873 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1824/ 2648, ite: 228] train loss: 5.213580, tar: 0.523128 ]\n",
      "[Valid loss: 156.234472, tar: 15.417909 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1832/ 2648, ite: 229] train loss: 5.201552, tar: 0.521758 ]\n",
      "[Valid loss: 156.866749, tar: 15.480259 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1840/ 2648, ite: 230] train loss: 5.198307, tar: 0.521634 ]\n",
      "[Valid loss: 157.499550, tar: 15.542993 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1848/ 2648, ite: 231] train loss: 5.194259, tar: 0.521130 ]\n",
      "[Valid loss: 158.123368, tar: 15.604971 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1856/ 2648, ite: 232] train loss: 5.183148, tar: 0.519806 ]\n",
      "[Valid loss: 158.755470, tar: 15.667909 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1864/ 2648, ite: 233] train loss: 5.183187, tar: 0.519958 ]\n",
      "[Valid loss: 159.391941, tar: 15.730969 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1872/ 2648, ite: 234] train loss: 5.174016, tar: 0.518828 ]\n",
      "[Valid loss: 160.009988, tar: 15.791708 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1880/ 2648, ite: 235] train loss: 5.168114, tar: 0.518048 ]\n",
      "[Valid loss: 160.630502, tar: 15.852174 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1888/ 2648, ite: 236] train loss: 5.164934, tar: 0.517672 ]\n",
      "[Valid loss: 161.231349, tar: 15.910452 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1896/ 2648, ite: 237] train loss: 5.190568, tar: 0.520902 ]\n",
      "[Valid loss: 161.857789, tar: 15.971841 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1904/ 2648, ite: 238] train loss: 5.182404, tar: 0.520037 ]\n",
      "[Valid loss: 162.467696, tar: 16.031613 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1912/ 2648, ite: 239] train loss: 5.193997, tar: 0.521366 ]\n",
      "[Valid loss: 163.091127, tar: 16.093019 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1920/ 2648, ite: 240] train loss: 5.182948, tar: 0.519993 ]\n",
      "[Valid loss: 163.716135, tar: 16.154627 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1928/ 2648, ite: 241] train loss: 5.167541, tar: 0.518309 ]\n",
      "[Valid loss: 164.342737, tar: 16.216410 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1936/ 2648, ite: 242] train loss: 5.159824, tar: 0.517455 ]\n",
      "[Valid loss: 164.971472, tar: 16.279268 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1944/ 2648, ite: 243] train loss: 5.150271, tar: 0.516335 ]\n",
      "[Valid loss: 165.598169, tar: 16.341351 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1952/ 2648, ite: 244] train loss: 5.154496, tar: 0.516813 ]\n",
      "[Valid loss: 166.220150, tar: 16.402564 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1960/ 2648, ite: 245] train loss: 5.151428, tar: 0.516712 ]\n",
      "[Valid loss: 166.848444, tar: 16.464922 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1968/ 2648, ite: 246] train loss: 5.145846, tar: 0.516160 ]\n",
      "[Valid loss: 167.474641, tar: 16.526721 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1976/ 2648, ite: 247] train loss: 5.138624, tar: 0.515337 ]\n",
      "[Valid loss: 168.128463, tar: 16.591501 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1984/ 2648, ite: 248] train loss: 5.140775, tar: 0.515610 ]\n",
      "[Valid loss: 168.755985, tar: 16.653319 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  1992/ 2648, ite: 249] train loss: 5.147587, tar: 0.516332 ]\n",
      "[Valid loss: 169.377412, tar: 16.715118 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2000/ 2648, ite: 250] train loss: 5.149319, tar: 0.516374 ]\n",
      "[Valid loss: 169.992275, tar: 16.775548 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2008/ 2648, ite: 251] train loss: 5.142585, tar: 0.515525 ]\n",
      "[Valid loss: 170.597444, tar: 16.834912 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2016/ 2648, ite: 252] train loss: 5.137477, tar: 0.514867 ]\n",
      "[Valid loss: 171.207590, tar: 16.895251 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2024/ 2648, ite: 253] train loss: 5.132197, tar: 0.514195 ]\n",
      "[Valid loss: 171.817586, tar: 16.955608 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2032/ 2648, ite: 254] train loss: 5.143385, tar: 0.515502 ]\n",
      "[Valid loss: 172.424933, tar: 17.014826 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2040/ 2648, ite: 255] train loss: 5.138227, tar: 0.514910 ]\n",
      "[Valid loss: 173.035671, tar: 17.074781 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2048/ 2648, ite: 256] train loss: 5.131126, tar: 0.514100 ]\n",
      "[Valid loss: 173.657138, tar: 17.136628 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2056/ 2648, ite: 257] train loss: 5.123070, tar: 0.513126 ]\n",
      "[Valid loss: 174.261901, tar: 17.196086 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2064/ 2648, ite: 258] train loss: 5.115208, tar: 0.512153 ]\n",
      "[Valid loss: 174.864268, tar: 17.255166 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2072/ 2648, ite: 259] train loss: 5.109839, tar: 0.511502 ]\n",
      "[Valid loss: 175.469104, tar: 17.314646 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2080/ 2648, ite: 260] train loss: 5.103432, tar: 0.510691 ]\n",
      "[Valid loss: 176.076707, tar: 17.374628 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2088/ 2648, ite: 261] train loss: 5.101268, tar: 0.510509 ]\n",
      "[Valid loss: 176.681019, tar: 17.434081 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2096/ 2648, ite: 262] train loss: 5.098167, tar: 0.510044 ]\n",
      "[Valid loss: 177.281985, tar: 17.492985 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2104/ 2648, ite: 263] train loss: 5.091610, tar: 0.509405 ]\n",
      "[Valid loss: 177.880711, tar: 17.551813 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2112/ 2648, ite: 264] train loss: 5.080664, tar: 0.508086 ]\n",
      "[Valid loss: 178.482138, tar: 17.611109 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2120/ 2648, ite: 265] train loss: 5.081156, tar: 0.508131 ]\n",
      "[Valid loss: 179.078076, tar: 17.669985 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2128/ 2648, ite: 266] train loss: 5.093011, tar: 0.509409 ]\n",
      "[Valid loss: 179.671995, tar: 17.728449 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2136/ 2648, ite: 267] train loss: 5.100957, tar: 0.510524 ]\n",
      "[Valid loss: 180.279392, tar: 17.789032 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2144/ 2648, ite: 268] train loss: 5.103401, tar: 0.510871 ]\n",
      "[Valid loss: 180.881707, tar: 17.848978 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2152/ 2648, ite: 269] train loss: 5.108766, tar: 0.511020 ]\n",
      "[Valid loss: 181.483779, tar: 17.908819 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2160/ 2648, ite: 270] train loss: 5.112496, tar: 0.511394 ]\n",
      "[Valid loss: 182.088260, tar: 17.969392 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2168/ 2648, ite: 271] train loss: 5.105737, tar: 0.510568 ]\n",
      "[Valid loss: 182.688157, tar: 18.029641 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2176/ 2648, ite: 272] train loss: 5.106908, tar: 0.510494 ]\n",
      "[Valid loss: 183.296622, tar: 18.091180 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2184/ 2648, ite: 273] train loss: 5.098294, tar: 0.509566 ]\n",
      "[Valid loss: 183.909937, tar: 18.153264 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2192/ 2648, ite: 274] train loss: 5.104357, tar: 0.510017 ]\n",
      "[Valid loss: 184.513401, tar: 18.214703 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2200/ 2648, ite: 275] train loss: 5.100919, tar: 0.509686 ]\n",
      "[Valid loss: 185.130516, tar: 18.277987 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2208/ 2648, ite: 276] train loss: 5.096569, tar: 0.509244 ]\n",
      "[Valid loss: 185.745440, tar: 18.341007 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2216/ 2648, ite: 277] train loss: 5.093791, tar: 0.509025 ]\n",
      "[Valid loss: 186.351987, tar: 18.402053 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2224/ 2648, ite: 278] train loss: 5.086691, tar: 0.508224 ]\n",
      "[Valid loss: 186.958233, tar: 18.462962 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2232/ 2648, ite: 279] train loss: 5.087026, tar: 0.508150 ]\n",
      "[Valid loss: 187.562467, tar: 18.523444 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2240/ 2648, ite: 280] train loss: 5.087547, tar: 0.508250 ]\n",
      "[Valid loss: 188.163972, tar: 18.583284 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2248/ 2648, ite: 281] train loss: 5.086351, tar: 0.508169 ]\n",
      "[Valid loss: 188.768630, tar: 18.643224 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2256/ 2648, ite: 282] train loss: 5.089758, tar: 0.508668 ]\n",
      "[Valid loss: 189.380915, tar: 18.704376 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2264/ 2648, ite: 283] train loss: 5.093805, tar: 0.509326 ]\n",
      "[Valid loss: 189.991500, tar: 18.764934 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2272/ 2648, ite: 284] train loss: 5.098283, tar: 0.509986 ]\n",
      "[Valid loss: 190.588698, tar: 18.824289 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2280/ 2648, ite: 285] train loss: 5.116939, tar: 0.512104 ]\n",
      "[Valid loss: 191.177050, tar: 18.882576 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2288/ 2648, ite: 286] train loss: 5.123134, tar: 0.512856 ]\n",
      "[Valid loss: 191.782202, tar: 18.942757 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2296/ 2648, ite: 287] train loss: 5.115151, tar: 0.511909 ]\n",
      "[Valid loss: 192.386377, tar: 19.002848 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2304/ 2648, ite: 288] train loss: 5.122520, tar: 0.512752 ]\n",
      "[Valid loss: 192.987261, tar: 19.062585 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2312/ 2648, ite: 289] train loss: 5.114736, tar: 0.511866 ]\n",
      "[Valid loss: 193.580071, tar: 19.121567 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2320/ 2648, ite: 290] train loss: 5.116274, tar: 0.512032 ]\n",
      "[Valid loss: 194.186837, tar: 19.181991 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2328/ 2648, ite: 291] train loss: 5.113817, tar: 0.511757 ]\n",
      "[Valid loss: 194.788164, tar: 19.242135 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2336/ 2648, ite: 292] train loss: 5.111808, tar: 0.511568 ]\n",
      "[Valid loss: 195.394325, tar: 19.302694 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2344/ 2648, ite: 293] train loss: 5.104831, tar: 0.510773 ]\n",
      "[Valid loss: 195.991425, tar: 19.361606 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2352/ 2648, ite: 294] train loss: 5.112559, tar: 0.511729 ]\n",
      "[Valid loss: 196.586244, tar: 19.419968 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2360/ 2648, ite: 295] train loss: 5.111516, tar: 0.511659 ]\n",
      "[Valid loss: 197.183901, tar: 19.479090 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2368/ 2648, ite: 296] train loss: 5.108934, tar: 0.511217 ]\n",
      "[Valid loss: 197.767945, tar: 19.536568 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2376/ 2648, ite: 297] train loss: 5.101683, tar: 0.510380 ]\n",
      "[Valid loss: 198.358328, tar: 19.594808 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2384/ 2648, ite: 298] train loss: 5.096703, tar: 0.509819 ]\n",
      "[Valid loss: 198.949581, tar: 19.653657 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2392/ 2648, ite: 299] train loss: 5.088082, tar: 0.508873 ]\n",
      "[Valid loss: 199.541433, tar: 19.712562 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2400/ 2648, ite: 300] train loss: 5.083230, tar: 0.508348 ]\n",
      "[Valid loss: 200.128683, tar: 19.770756 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2408/ 2648, ite: 301] train loss: 5.078949, tar: 0.507836 ]\n",
      "[Valid loss: 200.729142, tar: 19.830265 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2416/ 2648, ite: 302] train loss: 5.079671, tar: 0.507911 ]\n",
      "[Valid loss: 201.319336, tar: 19.888525 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2424/ 2648, ite: 303] train loss: 5.082566, tar: 0.508517 ]\n",
      "[Valid loss: 201.911186, tar: 19.947218 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2432/ 2648, ite: 304] train loss: 5.075734, tar: 0.507770 ]\n",
      "[Valid loss: 202.522283, tar: 20.008174 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2440/ 2648, ite: 305] train loss: 5.070327, tar: 0.507120 ]\n",
      "[Valid loss: 203.136282, tar: 20.069395 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2448/ 2648, ite: 306] train loss: 5.063295, tar: 0.506287 ]\n",
      "[Valid loss: 203.752043, tar: 20.130932 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2456/ 2648, ite: 307] train loss: 5.060207, tar: 0.506116 ]\n",
      "[Valid loss: 204.352936, tar: 20.190604 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2464/ 2648, ite: 308] train loss: 5.053256, tar: 0.505098 ]\n",
      "[Valid loss: 204.968249, tar: 20.251850 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2472/ 2648, ite: 309] train loss: 5.043860, tar: 0.504026 ]\n",
      "[Valid loss: 205.581940, tar: 20.313132 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2480/ 2648, ite: 310] train loss: 5.048336, tar: 0.504244 ]\n",
      "[Valid loss: 206.199586, tar: 20.375053 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2488/ 2648, ite: 311] train loss: 5.050205, tar: 0.504496 ]\n",
      "[Valid loss: 206.811471, tar: 20.435935 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2496/ 2648, ite: 312] train loss: 5.051941, tar: 0.504747 ]\n",
      "[Valid loss: 207.412179, tar: 20.495400 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2504/ 2648, ite: 313] train loss: 5.047261, tar: 0.504335 ]\n",
      "[Valid loss: 208.012358, tar: 20.554965 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2512/ 2648, ite: 314] train loss: 5.046511, tar: 0.504185 ]\n",
      "[Valid loss: 208.602303, tar: 20.613042 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2520/ 2648, ite: 315] train loss: 5.044600, tar: 0.504090 ]\n",
      "[Valid loss: 209.207751, tar: 20.673186 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2528/ 2648, ite: 316] train loss: 5.046785, tar: 0.504301 ]\n",
      "[Valid loss: 209.810312, tar: 20.733296 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2536/ 2648, ite: 317] train loss: 5.040733, tar: 0.503559 ]\n",
      "[Valid loss: 210.409480, tar: 20.792787 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2544/ 2648, ite: 318] train loss: 5.038042, tar: 0.503310 ]\n",
      "[Valid loss: 211.016049, tar: 20.853616 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2552/ 2648, ite: 319] train loss: 5.037178, tar: 0.503147 ]\n",
      "[Valid loss: 211.604461, tar: 20.912007 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2560/ 2648, ite: 320] train loss: 5.029977, tar: 0.502340 ]\n",
      "[Valid loss: 212.194373, tar: 20.971062 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2568/ 2648, ite: 321] train loss: 5.037412, tar: 0.503379 ]\n",
      "[Valid loss: 212.796207, tar: 21.031179 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2576/ 2648, ite: 322] train loss: 5.028099, tar: 0.502298 ]\n",
      "[Valid loss: 213.390816, tar: 21.090442 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2584/ 2648, ite: 323] train loss: 5.019676, tar: 0.501407 ]\n",
      "[Valid loss: 213.991123, tar: 21.150399 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2592/ 2648, ite: 324] train loss: 5.014967, tar: 0.500940 ]\n",
      "[Valid loss: 214.606357, tar: 21.211547 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2600/ 2648, ite: 325] train loss: 5.015798, tar: 0.500906 ]\n",
      "[Valid loss: 215.205579, tar: 21.271020 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2608/ 2648, ite: 326] train loss: 5.019387, tar: 0.501387 ]\n",
      "[Valid loss: 215.799301, tar: 21.329639 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2616/ 2648, ite: 327] train loss: 5.018348, tar: 0.501359 ]\n",
      "[Valid loss: 216.405527, tar: 21.390226 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2624/ 2648, ite: 328] train loss: 5.021144, tar: 0.501691 ]\n",
      "[Valid loss: 217.009329, tar: 21.450393 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2632/ 2648, ite: 329] train loss: 5.014311, tar: 0.500871 ]\n",
      "[Valid loss: 217.614778, tar: 21.511186 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2640/ 2648, ite: 330] train loss: 5.020008, tar: 0.501537 ]\n",
      "[Valid loss: 218.231316, tar: 21.573287 ]\n",
      "*************\n",
      "[epoch:   1/200, batch:  2648/ 2648, ite: 331] train loss: 5.019636, tar: 0.501453 ]\n",
      "[Valid loss: 218.832374, tar: 21.633420 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:     8/ 2648, ite: 332] train loss: 5.013175, tar: 0.500647 ]\n",
      "[Valid loss: 219.442048, tar: 21.694149 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    16/ 2648, ite: 333] train loss: 5.018264, tar: 0.501408 ]\n",
      "[Valid loss: 220.037850, tar: 21.753517 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    24/ 2648, ite: 334] train loss: 5.012041, tar: 0.500750 ]\n",
      "[Valid loss: 220.646144, tar: 21.814082 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    32/ 2648, ite: 335] train loss: 5.006525, tar: 0.500081 ]\n",
      "[Valid loss: 221.240949, tar: 21.873244 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    40/ 2648, ite: 336] train loss: 5.011342, tar: 0.500698 ]\n",
      "[Valid loss: 221.843592, tar: 21.933298 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    48/ 2648, ite: 337] train loss: 5.005835, tar: 0.500045 ]\n",
      "[Valid loss: 222.441125, tar: 21.993203 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    56/ 2648, ite: 338] train loss: 5.012373, tar: 0.500683 ]\n",
      "[Valid loss: 223.038730, tar: 22.052951 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    64/ 2648, ite: 339] train loss: 5.005445, tar: 0.499972 ]\n",
      "[Valid loss: 223.654540, tar: 22.114726 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    72/ 2648, ite: 340] train loss: 5.000735, tar: 0.499476 ]\n",
      "[Valid loss: 224.256667, tar: 22.174203 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    80/ 2648, ite: 341] train loss: 4.995353, tar: 0.499003 ]\n",
      "[Valid loss: 224.852387, tar: 22.233300 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    88/ 2648, ite: 342] train loss: 4.995673, tar: 0.499097 ]\n",
      "[Valid loss: 225.453520, tar: 22.292424 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:    96/ 2648, ite: 343] train loss: 4.988855, tar: 0.498267 ]\n",
      "[Valid loss: 226.048835, tar: 22.351197 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   104/ 2648, ite: 344] train loss: 4.982426, tar: 0.497461 ]\n",
      "[Valid loss: 226.634571, tar: 22.408627 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   112/ 2648, ite: 345] train loss: 4.990598, tar: 0.498132 ]\n",
      "[Valid loss: 227.234474, tar: 22.467992 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   120/ 2648, ite: 346] train loss: 4.984863, tar: 0.497343 ]\n",
      "[Valid loss: 227.829930, tar: 22.526680 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   128/ 2648, ite: 347] train loss: 4.978517, tar: 0.496661 ]\n",
      "[Valid loss: 228.428987, tar: 22.586190 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   136/ 2648, ite: 348] train loss: 4.973182, tar: 0.495945 ]\n",
      "[Valid loss: 229.022327, tar: 22.645088 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   144/ 2648, ite: 349] train loss: 4.968387, tar: 0.495374 ]\n",
      "[Valid loss: 229.621237, tar: 22.704490 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   152/ 2648, ite: 350] train loss: 4.972167, tar: 0.495975 ]\n",
      "[Valid loss: 230.197892, tar: 22.761235 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   160/ 2648, ite: 351] train loss: 4.969736, tar: 0.495696 ]\n",
      "[Valid loss: 230.782200, tar: 22.819168 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   168/ 2648, ite: 352] train loss: 4.962007, tar: 0.494811 ]\n",
      "[Valid loss: 231.367544, tar: 22.876887 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   176/ 2648, ite: 353] train loss: 4.956272, tar: 0.494209 ]\n",
      "[Valid loss: 231.954440, tar: 22.935393 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   184/ 2648, ite: 354] train loss: 4.954911, tar: 0.494511 ]\n",
      "[Valid loss: 232.540984, tar: 22.993488 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   192/ 2648, ite: 355] train loss: 4.950529, tar: 0.493895 ]\n",
      "[Valid loss: 233.137961, tar: 23.052840 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   200/ 2648, ite: 356] train loss: 4.947078, tar: 0.493391 ]\n",
      "[Valid loss: 233.729980, tar: 23.111653 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   208/ 2648, ite: 357] train loss: 4.941510, tar: 0.492716 ]\n",
      "[Valid loss: 234.350507, tar: 23.173976 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   216/ 2648, ite: 358] train loss: 4.937347, tar: 0.492247 ]\n",
      "[Valid loss: 234.959866, tar: 23.235191 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   224/ 2648, ite: 359] train loss: 4.937864, tar: 0.492440 ]\n",
      "[Valid loss: 235.582345, tar: 23.297946 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   232/ 2648, ite: 360] train loss: 4.931424, tar: 0.491662 ]\n",
      "[Valid loss: 238.098024, tar: 23.549030 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   264/ 2648, ite: 364] train loss: 4.915214, tar: 0.489965 ]\n",
      "[Valid loss: 238.714590, tar: 23.609142 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   272/ 2648, ite: 365] train loss: 4.908793, tar: 0.489129 ]\n",
      "[Valid loss: 239.318319, tar: 23.668107 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   280/ 2648, ite: 366] train loss: 4.910408, tar: 0.489163 ]\n",
      "[Valid loss: 239.910854, tar: 23.725565 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   288/ 2648, ite: 367] train loss: 4.908794, tar: 0.488892 ]\n",
      "[Valid loss: 240.511756, tar: 23.784435 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   296/ 2648, ite: 368] train loss: 4.903545, tar: 0.488405 ]\n",
      "[Valid loss: 241.112661, tar: 23.843569 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   304/ 2648, ite: 369] train loss: 4.903118, tar: 0.488230 ]\n",
      "[Valid loss: 241.719448, tar: 23.904510 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   312/ 2648, ite: 370] train loss: 4.901327, tar: 0.487981 ]\n",
      "[Valid loss: 242.334524, tar: 23.966295 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   320/ 2648, ite: 371] train loss: 4.896346, tar: 0.487482 ]\n",
      "[Valid loss: 242.953478, tar: 24.028168 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   328/ 2648, ite: 372] train loss: 4.893874, tar: 0.487170 ]\n",
      "[Valid loss: 243.583532, tar: 24.091204 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   336/ 2648, ite: 373] train loss: 4.886571, tar: 0.486369 ]\n",
      "[Valid loss: 244.219201, tar: 24.155066 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   344/ 2648, ite: 374] train loss: 4.880824, tar: 0.485721 ]\n",
      "[Valid loss: 244.841607, tar: 24.217146 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   352/ 2648, ite: 375] train loss: 4.880470, tar: 0.485527 ]\n",
      "[Valid loss: 245.479145, tar: 24.281262 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   360/ 2648, ite: 376] train loss: 4.879770, tar: 0.485330 ]\n",
      "[Valid loss: 246.090858, tar: 24.341774 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   368/ 2648, ite: 377] train loss: 4.877102, tar: 0.484854 ]\n",
      "[Valid loss: 246.713238, tar: 24.403569 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   376/ 2648, ite: 378] train loss: 4.867956, tar: 0.483843 ]\n",
      "[Valid loss: 247.325661, tar: 24.463856 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   384/ 2648, ite: 379] train loss: 4.861013, tar: 0.483144 ]\n",
      "[Valid loss: 247.946216, tar: 24.525337 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   392/ 2648, ite: 380] train loss: 4.855860, tar: 0.482542 ]\n",
      "[Valid loss: 248.551931, tar: 24.585344 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   400/ 2648, ite: 381] train loss: 4.869045, tar: 0.483944 ]\n",
      "[Valid loss: 249.173891, tar: 24.647436 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   408/ 2648, ite: 382] train loss: 4.873969, tar: 0.484497 ]\n",
      "[Valid loss: 249.789687, tar: 24.708828 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   416/ 2648, ite: 383] train loss: 4.868242, tar: 0.483846 ]\n",
      "[Valid loss: 250.403111, tar: 24.769885 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   424/ 2648, ite: 384] train loss: 4.873296, tar: 0.484261 ]\n",
      "[Valid loss: 251.022592, tar: 24.831705 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   432/ 2648, ite: 385] train loss: 4.869916, tar: 0.483839 ]\n",
      "[Valid loss: 251.629324, tar: 24.891598 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   440/ 2648, ite: 386] train loss: 4.871086, tar: 0.483992 ]\n",
      "[Valid loss: 252.249361, tar: 24.953378 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   448/ 2648, ite: 387] train loss: 4.872996, tar: 0.484279 ]\n",
      "[Valid loss: 252.867093, tar: 25.014917 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   456/ 2648, ite: 388] train loss: 4.874696, tar: 0.484484 ]\n",
      "[Valid loss: 253.481194, tar: 25.075711 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   464/ 2648, ite: 389] train loss: 4.870933, tar: 0.483987 ]\n",
      "[Valid loss: 254.093381, tar: 25.136424 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   472/ 2648, ite: 390] train loss: 4.869493, tar: 0.483836 ]\n",
      "[Valid loss: 254.698048, tar: 25.195716 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   480/ 2648, ite: 391] train loss: 4.868227, tar: 0.483719 ]\n",
      "[Valid loss: 255.299695, tar: 25.254262 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   488/ 2648, ite: 392] train loss: 4.865556, tar: 0.483464 ]\n",
      "[Valid loss: 255.899510, tar: 25.312257 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   496/ 2648, ite: 393] train loss: 4.864870, tar: 0.483358 ]\n",
      "[Valid loss: 256.497665, tar: 25.370657 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   504/ 2648, ite: 394] train loss: 4.858935, tar: 0.482706 ]\n",
      "[Valid loss: 257.086401, tar: 25.427726 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   512/ 2648, ite: 395] train loss: 4.859394, tar: 0.482808 ]\n",
      "[Valid loss: 257.679474, tar: 25.485370 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   520/ 2648, ite: 396] train loss: 4.853155, tar: 0.482125 ]\n",
      "[Valid loss: 258.265079, tar: 25.542552 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   528/ 2648, ite: 397] train loss: 4.848460, tar: 0.481526 ]\n",
      "[Valid loss: 258.853484, tar: 25.600465 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   536/ 2648, ite: 398] train loss: 4.844726, tar: 0.481065 ]\n",
      "[Valid loss: 259.437389, tar: 25.657823 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   544/ 2648, ite: 399] train loss: 4.840721, tar: 0.480655 ]\n",
      "[Valid loss: 260.032119, tar: 25.716699 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   552/ 2648, ite: 400] train loss: 4.841809, tar: 0.480742 ]\n",
      "[Valid loss: 260.631061, tar: 25.775816 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   560/ 2648, ite: 401] train loss: 4.837167, tar: 0.480170 ]\n",
      "[Valid loss: 261.238289, tar: 25.836101 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   568/ 2648, ite: 402] train loss: 4.833815, tar: 0.479793 ]\n",
      "[Valid loss: 261.829816, tar: 25.894144 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   576/ 2648, ite: 403] train loss: 4.830261, tar: 0.479352 ]\n",
      "[Valid loss: 262.410712, tar: 25.950930 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   584/ 2648, ite: 404] train loss: 4.832261, tar: 0.479540 ]\n",
      "[Valid loss: 263.007423, tar: 26.009738 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   592/ 2648, ite: 405] train loss: 4.833526, tar: 0.479667 ]\n",
      "[Valid loss: 263.606858, tar: 26.069176 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   600/ 2648, ite: 406] train loss: 4.829516, tar: 0.479103 ]\n",
      "[Valid loss: 264.203078, tar: 26.128367 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   608/ 2648, ite: 407] train loss: 4.827404, tar: 0.478833 ]\n",
      "[Valid loss: 264.802359, tar: 26.187961 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   616/ 2648, ite: 408] train loss: 4.827759, tar: 0.478864 ]\n",
      "[Valid loss: 265.401033, tar: 26.248077 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   624/ 2648, ite: 409] train loss: 4.824342, tar: 0.478518 ]\n",
      "[Valid loss: 266.019080, tar: 26.310922 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   632/ 2648, ite: 410] train loss: 4.820489, tar: 0.478173 ]\n",
      "[Valid loss: 266.658896, tar: 26.376618 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   640/ 2648, ite: 411] train loss: 4.814554, tar: 0.477458 ]\n",
      "[Valid loss: 267.304580, tar: 26.443975 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   648/ 2648, ite: 412] train loss: 4.813181, tar: 0.477380 ]\n",
      "[Valid loss: 267.961069, tar: 26.512965 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   656/ 2648, ite: 413] train loss: 4.804766, tar: 0.476456 ]\n",
      "[Valid loss: 268.628241, tar: 26.583066 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   664/ 2648, ite: 414] train loss: 4.799556, tar: 0.475883 ]\n",
      "[Valid loss: 269.285442, tar: 26.652186 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   672/ 2648, ite: 415] train loss: 4.793523, tar: 0.475090 ]\n",
      "[Valid loss: 269.963475, tar: 26.724283 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   680/ 2648, ite: 416] train loss: 4.789511, tar: 0.474772 ]\n",
      "[Valid loss: 270.610801, tar: 26.791994 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   688/ 2648, ite: 417] train loss: 4.800354, tar: 0.475971 ]\n",
      "[Valid loss: 271.263069, tar: 26.859244 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   696/ 2648, ite: 418] train loss: 4.803684, tar: 0.476455 ]\n",
      "[Valid loss: 271.875814, tar: 26.920745 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   704/ 2648, ite: 419] train loss: 4.797712, tar: 0.475748 ]\n",
      "[Valid loss: 272.470100, tar: 26.979261 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   712/ 2648, ite: 420] train loss: 4.798532, tar: 0.475886 ]\n",
      "[Valid loss: 273.068772, tar: 27.037651 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   720/ 2648, ite: 421] train loss: 4.796494, tar: 0.475669 ]\n",
      "[Valid loss: 273.659089, tar: 27.095272 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   728/ 2648, ite: 422] train loss: 4.797888, tar: 0.475816 ]\n",
      "[Valid loss: 274.253625, tar: 27.153568 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   736/ 2648, ite: 423] train loss: 4.793815, tar: 0.475442 ]\n",
      "[Valid loss: 274.851184, tar: 27.212795 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   744/ 2648, ite: 424] train loss: 4.787585, tar: 0.474788 ]\n",
      "[Valid loss: 275.444444, tar: 27.271686 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   752/ 2648, ite: 425] train loss: 4.783068, tar: 0.474252 ]\n",
      "[Valid loss: 276.043374, tar: 27.330853 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   760/ 2648, ite: 426] train loss: 4.784411, tar: 0.474498 ]\n",
      "[Valid loss: 276.648786, tar: 27.390991 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   768/ 2648, ite: 427] train loss: 4.786321, tar: 0.474681 ]\n",
      "[Valid loss: 277.242020, tar: 27.449933 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   776/ 2648, ite: 428] train loss: 4.785154, tar: 0.474650 ]\n",
      "[Valid loss: 277.856207, tar: 27.511733 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   784/ 2648, ite: 429] train loss: 4.783251, tar: 0.474443 ]\n",
      "[Valid loss: 278.464336, tar: 27.572026 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   792/ 2648, ite: 430] train loss: 4.778632, tar: 0.474003 ]\n",
      "[Valid loss: 279.069706, tar: 27.631907 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   800/ 2648, ite: 431] train loss: 4.781624, tar: 0.474536 ]\n",
      "[Valid loss: 279.675034, tar: 27.691659 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   808/ 2648, ite: 432] train loss: 4.779033, tar: 0.474290 ]\n",
      "[Valid loss: 280.265827, tar: 27.748745 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   816/ 2648, ite: 433] train loss: 4.781750, tar: 0.474682 ]\n",
      "[Valid loss: 280.851584, tar: 27.805556 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   824/ 2648, ite: 434] train loss: 4.779129, tar: 0.474393 ]\n",
      "[Valid loss: 281.421566, tar: 27.860777 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   832/ 2648, ite: 435] train loss: 4.778223, tar: 0.474186 ]\n",
      "[Valid loss: 282.006264, tar: 27.918214 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   840/ 2648, ite: 436] train loss: 4.786352, tar: 0.475225 ]\n",
      "[Valid loss: 282.596868, tar: 27.976729 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   848/ 2648, ite: 437] train loss: 4.780672, tar: 0.474612 ]\n",
      "[Valid loss: 283.177593, tar: 28.034357 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   856/ 2648, ite: 438] train loss: 4.778607, tar: 0.474294 ]\n",
      "[Valid loss: 283.773515, tar: 28.093802 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   864/ 2648, ite: 439] train loss: 4.777472, tar: 0.474208 ]\n",
      "[Valid loss: 284.359142, tar: 28.151628 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   872/ 2648, ite: 440] train loss: 4.774230, tar: 0.473795 ]\n",
      "[Valid loss: 284.949267, tar: 28.209738 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   880/ 2648, ite: 441] train loss: 4.771034, tar: 0.473367 ]\n",
      "[Valid loss: 285.533188, tar: 28.267632 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   888/ 2648, ite: 442] train loss: 4.768660, tar: 0.473152 ]\n",
      "[Valid loss: 286.109401, tar: 28.323710 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   896/ 2648, ite: 443] train loss: 4.764962, tar: 0.472704 ]\n",
      "[Valid loss: 286.688470, tar: 28.380212 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   904/ 2648, ite: 444] train loss: 4.760537, tar: 0.472188 ]\n",
      "[Valid loss: 287.279361, tar: 28.437595 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   912/ 2648, ite: 445] train loss: 4.756097, tar: 0.471682 ]\n",
      "[Valid loss: 287.865784, tar: 28.495104 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   920/ 2648, ite: 446] train loss: 4.753734, tar: 0.471443 ]\n",
      "[Valid loss: 288.448546, tar: 28.551735 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   928/ 2648, ite: 447] train loss: 4.748751, tar: 0.470907 ]\n",
      "[Valid loss: 289.041809, tar: 28.609581 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   936/ 2648, ite: 448] train loss: 4.748611, tar: 0.470880 ]\n",
      "[Valid loss: 289.625683, tar: 28.666499 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   944/ 2648, ite: 449] train loss: 4.749658, tar: 0.471037 ]\n",
      "[Valid loss: 290.223291, tar: 28.724933 ]\n",
      "*************\n",
      "[epoch:   2/200, batch:   952/ 2648, ite: 450] train loss: 4.746565, tar: 0.470579 ]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, epoch_num):\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6, d7 = net(inputs_v)\n",
    "        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_tar_loss += loss2.item()\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, d7, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f ]\" % (\n",
    "        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_size, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "        \n",
    "        \n",
    "        net.eval()\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            \n",
    "            ite_num_valid = ite_num + 1\n",
    "            ite_num4val_valid = ite_num4val + 1\n",
    "\n",
    "            inputs, labels = data['image'], data['label']\n",
    "        \n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                            requires_grad=False)\n",
    "            else:\n",
    "                inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            d0, d1, d2, d3, d4, d5, d6, d7 = net(inputs_v)\n",
    "            loss2_val, loss_val = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v)\n",
    "            \n",
    "            \n",
    "            # # print statistics\n",
    "            running_loss_valid += loss_val.item()\n",
    "            running_tar_loss_valid += loss2_val.item()\n",
    "\n",
    "        \n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, d7, loss2_val, loss_val\n",
    "        \n",
    "        valid_loss = running_tar_loss_valid / len(valid_loader.dataset)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"[Valid loss: %3f, tar: %3f ]\" % (running_loss_valid / len(valid_loader.dataset),\n",
    "                                               valid_loss))\n",
    "\n",
    "        print('*************')\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(net.state_dict(), model_dir + 'optimized_model.pth')\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_tar_loss = 0.0\n",
    "            net.train()  # resume train\n",
    "            ite_num4val = 0\n",
    "            print('##########')\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        #if ite_num % 2000 == 0:  # save model every 2000 iterations\n",
    "\n",
    "         #   torch.save(net.state_dict(), model_dir + \"basnet_bsi_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "         #   running_loss = 0.0\n",
    "         #   running_tar_loss = 0.0\n",
    "         #   net.train()  # resume train\n",
    "         #   ite_num4val = 0\n",
    "        \n",
    "        \n",
    "        #if ite_num % 2000 == 0:  # save model every 2000 iterations\n",
    "\n",
    "            #torch.save(net.state_dict(), model_dir + \"basnet_bsi_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "            #running_loss = 0.0\n",
    "            #running_tar_loss = 0.0\n",
    "           \n",
    "            #ite_num4val = 0\n",
    "\n",
    "print('-------------Congratulations! Training Done!!!-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
